{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-VGVY9yZqhny","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"source":"# **imort libraries**\n\n***导入第三方库***"},{"cell_type":"code","metadata":{"id":"mX0yw-q5dj02","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\nimport math\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torch.utils.data as data\nimport torchvision\nfrom torchvision import transforms\n\nfrom keras.layers import Input, Conv2D, MaxPooling2D\nfrom keras.layers import Dense, Flatten\nfrom keras.models import Model\nimport matplotlib.pyplot as plt\nimport pickle","execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PyJTO0h6wH3k","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"source":"**Import the data**\n\n***对数据进行导入***"},{"cell_type":"code","metadata":{"id":"M2Uuokd5wQhn","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"source":"PATH = os.path.join('/home/mw/input/', 'dataset')","execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T6L0yoVXIB7p","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"source":"**The data set has the following directory structure:**\n\n***数据集具有以下目录结构：***\n```dataset\n|__ train\n    |______ real: []\n    |______ fake: []\n|__ test\n    |______ real: []\n    |______ fake: []\n```\n**After extracting its content, assign appropriate file paths to the variables for training and validation sets.**\n\n***提取其内容后，为变量分配适当的文件路径以用于训练和验证集。***\n"},{"cell_type":"code","metadata":{"id":"2iE01qpDIij8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"source":"train_dir = os.path.join(PATH, 'train')\ntest_dir = os.path.join(PATH, 'validation')\n\ntrain_real_dir = os.path.join(train_dir, 'real') \ntrain_fake_dir = os.path.join(train_dir, 'fake')  \ntest_real_dir = os.path.join(test_dir, 'real')  \ntest_fake_dir = os.path.join(test_dir, 'fake')","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XW7NMVrH0w49","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"source":"# **get to know the dataset**\n\n***了解数据集***\n\n**Let's see how many real and fake images are in the training and validation catalog:**\n\n***让我们看看训练和验证目录中有多少真实和虚假的图像：***"},{"cell_type":"code","metadata":{"id":"PE9k6WtaJCeC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"77b74b3e-4ab6-47e0-b241-855e0429134b","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"source":"num_real_tr = len(os.listdir(train_real_dir))\nnum_fake_tr = len(os.listdir(train_fake_dir))\n\nnum_real_test = len(os.listdir(test_real_dir))\nnum_fake_test = len(os.listdir(test_fake_dir))\n\ntotal_train = num_real_tr + num_fake_tr\ntotal_test = num_real_test + num_fake_test\n\nprint('total training real images:', num_real_tr)\nprint('total training fake images:', num_fake_tr)\n\nprint('total test real images:', num_real_test)\nprint('total test fake images:', num_fake_test)\nprint(\"--\")\nprint(\"Total training images:\", total_train)\nprint(\"Total test images:\", total_test)","execution_count":4,"outputs":[{"output_type":"stream","text":"total training real images: 3613\ntotal training fake images: 5866\ntotal test real images: 1548\ntotal test fake images: 2513\n--\nTotal training images: 9479\nTotal test images: 4061\n","name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Wcd6joxnKC0W","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"source":"# 资料准备\n将图像格式化为经过适当预处理的浮点张量，然后再馈入网络：\n\n1. 从磁盘读取图像\n2. 解码这些图像的内同，并根据其RGB内容将其转换为正确的网格格式\n3. 将它们转换为浮点张量\n4. 将张量从0到255之间的值重新缩放为0到1之间的值，因为神经网络更喜欢处理较小的输入值\n\n\n解码这些图像的内容，并根据其RGB内容将其转换为正确的网格格式。\n将它们转换为浮点张量。\n将张量从0到255之间的值重新缩放为0到1之间的值，因为神经网络更喜欢处理较小的输入值。\n幸运的是，所有这些任务都可以通过提供的ImageDataGenerator类来完成tf.keras。它可以从磁盘读取图像并将其预处理为适当的张量。它还将设置将这些图像转换成张量的生成器，这对于训练网络很有帮助。"},{"cell_type":"markdown","metadata":{"id":"loyk0c-VnRdI","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"source":"训练量很少时，通常会发生过度拟合。解决此问题的一种方法是扩充数据集，使其具有足够数量的训练示例。数据增强采用通过使用产生真实感图像的随机变换增强样本来从现有训练样本生成更多训练数据的方法。目标是模型在训练期间永远不会看到两次完全相同的图片。这有助于使模型暴露于数据的更多方面，并且可以更好地进行概括。\n\ntf.keras使用ImageDataGenerator类来实现这一点。将不同的转换传递给数据集，它将在训练过程中加以应用。\n对训练图像进行重新缩放，45度旋转，宽度偏移，高度偏移，水平翻转和缩放增强。防止出现过拟合的情况"},{"cell_type":"code","metadata":{"id":"Uv3dtRYbTXXv","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"source":"BATCH_SIZE = 128\ntransform_method = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntrain_data = torchvision.datasets.ImageFolder(root=train_dir, transform=transform_method)\ntrain_data_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2)\ntest_data = torchvision.datasets.ImageFolder(root=test_dir, transform=transform_method)\ntest_data_loader  = data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2) \n\ntrainset = train_data\ntrainloader = train_data_loader\ntestset = test_data\ntestloader = test_data_loader","execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"c03haskrdj05","colab":{"base_uri":"https://localhost:8080/"},"outputId":"785621fd-6c59-4c1c-ebea-1921f0df11e3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"source":"print(\"Number of train samples: \", len(train_data))\nprint(\"Number of test samples: \", len(test_data))\nprint(\"Detected Classes are: \", train_data.class_to_idx) # classes are detected by folder structure\nprint(\"num classes = \", len(train_data.class_to_idx))","execution_count":6,"outputs":[{"output_type":"stream","text":"Number of train samples:  9479\nNumber of test samples:  4061\nDetected Classes are:  {'fake': 0, 'real': 1}\nnum classes =  2\n","name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hfLYQqJ1ug5T","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"source":"# 构建神经网络\n减少过度拟合的另一种技术是将丢失引入网络。这是一种正则化形式，它迫使网络中的权重仅取较小的值，这使得权重值的分配更加规则，并且网络可以减少在小的训练样本上的过度拟合。\n\n当您在图层上应用滤除时，它会在训练过程中从所应用的图层中随机滤除（设置为零）数量的输出单位。dropout采用分数形式作为其输入值，形式为0.1、0.2、0.4等。这意味着从所施加的层中随机退出输出单元的10％，20％或40％。将0.1的dropout应用于某个图层时，它会在每个训练时期随机杀死10％的输出单位。使用此新的dropout功能创建网络体系结构，并将其应用于不同的卷积和完全连接的层。"},{"cell_type":"code","metadata":{"id":"YFGeHtnVdj06","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"source":"class Net3(nn.Module):\n    def __init__(self):\n        super(Net3, self).__init__()\n        self.conv1 = nn.Conv2d(3, 28, 3,padding=1)\n        self.bn1 = nn.BatchNorm2d(28)\n        self.conv2 = nn.Conv2d(28, 35, 3,padding=1)\n        self.bn2 = nn.BatchNorm2d(35)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.drop1 = nn.Dropout(p=0.2)\n        self.conv3 = nn.Conv2d(35, 16, 3,padding=1)\n        self.bn3 = nn.BatchNorm2d(16)\n        \n        self.fc1 = nn.Linear(16 * 8 * 8, 100)\n        self.drop2 = nn.Dropout(p=0.3)\n        self.fc2 = nn.Linear(100, 30)\n        self.bn4 = nn.BatchNorm1d(30)\n        self.fc3 = nn.Linear(30, 10)\n\n    def forward(self, x):\n        x = self.bn1(F.relu(self.conv1(x)))\n        x = self.drop1(x)\n        x = self.bn2(F.relu(self.conv2(x)))\n        x = self.pool(x)\n        x = self.bn3(F.relu(self.conv3(x)))\n        x = self.pool(x)\n        x = x.view(-1, 16 * 8 * 8)\n        x = self.drop1(x)\n        x = F.relu(self.fc1(x))\n        x = self.drop2(x)\n        x = self.bn4(F.relu(self.fc2(x)))\n        x = self.fc3(x)\n    \n        return F.log_softmax(x, dim=0)\n","execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZU-CGW-Vdj07","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"source":"# Plot with Test Error"},{"cell_type":"code","metadata":{"id":"-eLWa1X-dj07","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"source":"def compute_test_acc(model, testloader):\n    # Test the model\n    model.eval()\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        count = 0\n        for images, labels in testloader:\n            count += 1\n            images = Variable(images)\n            labels = Variable(labels)\n\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    return (correct / total)\n\ndef compute_train_acc(model, trainloader):\n    # Test the model\n    model.eval()\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        count = 0\n        for images, labels in trainloader:\n            count += 1\n            images = Variable(images)\n            labels = Variable(labels)\n\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    return (correct / total)","execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"CbXGRZW2dj08","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"source":"# Find optimal_batch\ndef uncertainty_metric_max_min_difference_selection(model, trainloader):\n    batch_evaluation = []\n    for i, (images, labels) in enumerate(trainloader):\n        outputs = model(images)\n        batch_differences = []\n        for j in range(len(outputs)):\n            min_prob = min(outputs[j])\n            max_prob = max(outputs[j])\n            diff = float(max_prob - min_prob)\n            batch_differences.append(diff)\n        batch_mean = np.mean(batch_differences)\n        batch_evaluation.append(batch_mean)\n    batch_evaluation = np.array(batch_evaluation)\n    k = 20\n#     largest = list((-batch_evaluation).argsort()[:k])\n    smallest = list(np.argsort(batch_evaluation)[:k])\n    return smallest","execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"HldSUY-4dj08","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0dbf0255-63af-4604-aab4-35cc67569e4e","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"source":"loss_list = []\nacc_list = []\ntrain_err_list = []\ntest_err_list = []\n\nfor trial in range(1):\n    active_model = Net3()\n    free_params = sum(p.numel() for p in active_model.parameters() if p.requires_grad)\n    print(free_params)\n\n    trainstep = 125\n    # Loss and optimizer\n\n    criterion = nn.NLLLoss() #You can modify the loss function\n    optimizer = optim.SGD(active_model.parameters(), lr=0.005, momentum=0.9, weight_decay=8e-4) #You can change the optimizer\n\n    # Train the model\n\n    total_step = len(trainloader)\n    print(total_step)\n    trial_loss_list = []\n    trial_acc_list = []\n\n    trial_train_err_list = []\n    trial_test_err_list = []\n\n    num_epochs = 450\n\n    for epoch in range(num_epochs):\n        print(\"epoch: \", epoch)\n        total = 0\n        correct = 0\n        randomly_selected = uncertainty_metric_max_min_difference_selection(active_model, trainloader)\n        for i, (images, labels) in enumerate(trainloader):\n            if i not in randomly_selected:\n                continue\n\n    #         images = images.cuda(async=True)\n    #         labels = labels.cuda(async=True)\n\n            images = Variable(images)\n            labels = Variable(labels)\n            # Run the forward pass\n            # print(\"running forward pass....\")\n            outputs = active_model(images)\n\n            loss = criterion(outputs, labels)\n            trial_loss_list.append(loss.item())\n\n            # Backprop \n            # print(\"backpropagating......\")\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n             # Track the accuracy\n            total = labels.size(0) + total\n            _, predicted = torch.max(outputs.data, 1)\n            correct = (predicted == labels).sum().item() + correct\n            trial_acc_list.append(correct / total)\n\n\n            if (i + 1) % trainstep == 0:\n                w = torch.nn.utils.parameters_to_vector(active_model.parameters())\n                print(w)\n                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n                              (correct / total) * 100))\n        trial_train_err_list.append(compute_train_acc(active_model, trainloader))\n        trial_test_err_list.append(compute_test_acc(active_model, testloader))\n        if (total == correct):\n            break \n\n#     loss_list.append(np.array(trial_loss_list))\n#     acc_list.append(trial_acc_list)\n    train_err_list.append(trial_train_err_list)\n    test_err_list.append(trial_test_err_list)\n            \nprint('Finished Training') \n\n# loss_list = np.array(loss_list)\n# acc_list = np.array(acc_list)\ntrain_err_list= np.array(train_err_list)\ntest_err_list=np.array(test_err_list)\n\n# loss_list = np.mean(loss_list, axis=0)\n# acc_list = np.mean(acc_list, axis=0)\ntrain_err_list= np.mean(train_err_list, axis=0)\ntest_err_list=np.mean(test_err_list, axis=0)\n\n\n","execution_count":10,"outputs":[{"output_type":"stream","text":"120753\n75\nepoch:  0\nepoch:  1\nepoch:  2\nepoch:  3\nepoch:  4\nepoch:  5\nepoch:  6\nepoch:  7\nepoch:  8\nepoch:  9\nepoch:  10\nepoch:  11\nepoch:  12\nepoch:  13\nepoch:  14\nepoch:  15\nepoch:  16\nepoch:  17\nepoch:  18\nepoch:  19\nepoch:  20\nepoch:  21\nepoch:  22\nepoch:  23\nepoch:  24\nepoch:  25\nepoch:  26\nepoch:  27\nepoch:  28\nepoch:  29\nepoch:  30\nepoch:  31\nepoch:  32\nepoch:  33\nepoch:  34\nepoch:  35\nepoch:  36\nepoch:  37\nepoch:  38\nepoch:  39\nepoch:  40\nepoch:  41\nepoch:  42\nepoch:  43\nepoch:  44\nepoch:  45\nepoch:  46\nepoch:  47\nepoch:  48\nepoch:  49\nepoch:  50\nepoch:  51\nepoch:  52\nepoch:  53\nepoch:  54\nepoch:  55\nepoch:  56\nepoch:  57\nepoch:  58\nepoch:  59\nepoch:  60\nepoch:  61\nepoch:  62\nepoch:  63\nepoch:  64\nepoch:  65\nepoch:  66\nepoch:  67\nepoch:  68\nepoch:  69\nepoch:  70\nepoch:  71\nepoch:  72\nepoch:  73\nepoch:  74\nepoch:  75\nepoch:  76\nepoch:  77\nepoch:  78\nepoch:  79\nepoch:  80\nepoch:  81\nepoch:  82\nepoch:  83\nepoch:  84\nepoch:  85\nepoch:  86\nepoch:  87\nepoch:  88\nepoch:  89\nepoch:  90\nepoch:  91\nepoch:  92\nepoch:  93\nepoch:  94\nepoch:  95\nepoch:  96\nepoch:  97\nepoch:  98\nepoch:  99\nepoch:  100\nepoch:  101\nepoch:  102\nepoch:  103\nepoch:  104\nepoch:  105\nepoch:  106\nepoch:  107\nepoch:  108\nepoch:  109\nepoch:  110\nepoch:  111\nepoch:  112\nepoch:  113\nepoch:  114\nepoch:  115\nepoch:  116\nepoch:  117\nepoch:  118\nepoch:  119\nepoch:  120\nepoch:  121\nepoch:  122\nepoch:  123\nepoch:  124\nepoch:  125\nepoch:  126\nepoch:  127\nepoch:  128\nepoch:  129\nepoch:  130\nepoch:  131\nepoch:  132\nepoch:  133\nepoch:  134\nepoch:  135\nepoch:  136\nepoch:  137\nepoch:  138\nepoch:  139\nepoch:  140\nepoch:  141\nepoch:  142\nepoch:  143\nepoch:  144\nepoch:  145\nepoch:  146\nepoch:  147\nepoch:  148\nepoch:  149\nepoch:  150\nepoch:  151\nepoch:  152\nepoch:  153\nepoch:  154\nepoch:  155\nepoch:  156\nepoch:  157\nepoch:  158\nepoch:  159\nepoch:  160\nepoch:  161\nepoch:  162\nepoch:  163\nepoch:  164\nepoch:  165\nepoch:  166\nepoch:  167\nepoch:  168\nepoch:  169\nepoch:  170\nepoch:  171\nepoch:  172\nepoch:  173\nepoch:  174\nepoch:  175\nepoch:  176\nepoch:  177\nepoch:  178\nepoch:  179\nepoch:  180\nepoch:  181\nepoch:  182\nepoch:  183\nepoch:  184\nepoch:  185\nepoch:  186\nepoch:  187\nepoch:  188\nepoch:  189\nepoch:  190\nepoch:  191\nepoch:  192\nepoch:  193\nepoch:  194\nepoch:  195\nepoch:  196\nepoch:  197\nepoch:  198\nepoch:  199\nepoch:  200\nepoch:  201\nepoch:  202\nepoch:  203\nepoch:  204\nepoch:  205\nepoch:  206\nepoch:  207\nepoch:  208\nepoch:  209\nepoch:  210\nepoch:  211\nepoch:  212\nepoch:  213\nepoch:  214\nepoch:  215\nepoch:  216\nepoch:  217\nepoch:  218\nepoch:  219\nepoch:  220\nepoch:  221\nepoch:  222\nepoch:  223\nepoch:  224\nepoch:  225\nepoch:  226\nepoch:  227\nepoch:  228\nepoch:  229\nepoch:  230\nepoch:  231\nepoch:  232\nepoch:  233\nepoch:  234\nepoch:  235\nepoch:  236\nepoch:  237\nepoch:  238\nepoch:  239\nepoch:  240\nepoch:  241\nepoch:  242\nepoch:  243\nepoch:  244\nepoch:  245\nepoch:  246\nepoch:  247\nepoch:  248\nepoch:  249\nepoch:  250\nepoch:  251\nepoch:  252\nepoch:  253\nepoch:  254\nepoch:  255\nepoch:  256\nepoch:  257\nepoch:  258\nepoch:  259\nepoch:  260\nepoch:  261\nepoch:  262\nepoch:  263\nepoch:  264\nepoch:  265\nepoch:  266\nepoch:  267\nepoch:  268\nepoch:  269\nepoch:  270\nepoch:  271\nepoch:  272\nepoch:  273\nepoch:  274\nepoch:  275\nepoch:  276\nepoch:  277\nepoch:  278\nepoch:  279\nepoch:  280\nepoch:  281\nepoch:  282\nepoch:  283\nepoch:  284\nepoch:  285\nepoch:  286\nepoch:  287\nepoch:  288\nepoch:  289\nepoch:  290\nepoch:  291\nepoch:  292\nepoch:  293\nepoch:  294\nepoch:  295\nepoch:  296\nepoch:  297\nepoch:  298\nepoch:  299\nepoch:  300\nepoch:  301\nepoch:  302\nepoch:  303\nepoch:  304\nepoch:  305\nepoch:  306\nepoch:  307\nepoch:  308\nepoch:  309\nepoch:  310\nepoch:  311\nepoch:  312\nepoch:  313\nepoch:  314\nepoch:  315\nepoch:  316\nepoch:  317\nepoch:  318\nepoch:  319\nepoch:  320\nepoch:  321\nepoch:  322\nepoch:  323\nepoch:  324\nepoch:  325\nepoch:  326\nepoch:  327\nepoch:  328\nepoch:  329\nepoch:  330\nepoch:  331\nepoch:  332\nepoch:  333\nepoch:  334\nepoch:  335\nepoch:  336\nepoch:  337\nepoch:  338\nepoch:  339\nepoch:  340\nepoch:  341\nepoch:  342\nepoch:  343\nepoch:  344\nepoch:  345\nepoch:  346\nepoch:  347\nepoch:  348\nepoch:  349\nepoch:  350\nepoch:  351\nepoch:  352\nepoch:  353\nepoch:  354\nepoch:  355\nepoch:  356\nepoch:  357\nepoch:  358\nepoch:  359\nepoch:  360\nepoch:  361\nepoch:  362\nepoch:  363\nepoch:  364\nepoch:  365\nepoch:  366\nepoch:  367\nepoch:  368\nepoch:  369\nepoch:  370\nepoch:  371\nepoch:  372\nepoch:  373\nepoch:  374\nepoch:  375\nepoch:  376\nepoch:  377\nepoch:  378\nepoch:  379\nepoch:  380\nepoch:  381\nepoch:  382\nepoch:  383\nepoch:  384\nepoch:  385\nepoch:  386\nepoch:  387\nepoch:  388\nepoch:  389\nepoch:  390\nepoch:  391\nepoch:  392\nepoch:  393\nepoch:  394\nepoch:  395\nepoch:  396\nepoch:  397\nepoch:  398\nepoch:  399\nepoch:  400\nepoch:  401\nepoch:  402\nepoch:  403\nepoch:  404\nepoch:  405\nepoch:  406\nepoch:  407\nepoch:  408\nepoch:  409\nepoch:  410\nepoch:  411\nepoch:  412\nepoch:  413\nepoch:  414\nepoch:  415\nepoch:  416\nepoch:  417\nepoch:  418\nepoch:  419\nepoch:  420\nepoch:  421\nepoch:  422\nepoch:  423\nepoch:  424\nepoch:  425\nepoch:  426\nepoch:  427\nepoch:  428\nepoch:  429\nepoch:  430\nepoch:  431\nepoch:  432\nepoch:  433\nepoch:  434\nepoch:  435\nepoch:  436\nepoch:  437\nepoch:  438\nepoch:  439\nepoch:  440\nepoch:  441\nepoch:  442\nepoch:  443\nepoch:  444\nepoch:  445\nepoch:  446\nepoch:  447\nepoch:  448\nepoch:  449\nFinished Training\n","name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hi-EPHDNdj09","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"source":"# testing = []\n# testing.append([1,2,3])\n# testing.append([4,2,3])\n# testing = np.array(testing)\n# print(np.mean(testing, axis=0))","execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"1xV6rby5dj0-","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"source":"# ... after training, save your model \nmodel_filename = '/home/mw/work/active_model3_minimum.pt'\ntorch.save(active_model.state_dict(), model_filename)\n\n# .. to load your previously training model:\n# model2 = Net3()\n# model2.load_state_dict(torch.load(model_filename))\n# model2.eval()","execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"-MsHbyswdj0-","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"source":"active_train_err_list = train_err_list\nactive_test_err_list = test_err_list\nactive_acc_list1 = acc_list\n\n","execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Bbs4sosdj0-","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"source":"pickle_out = open(\"/home/mw/work/test1_largest_margin_minimum_training_accuracy.pickle\",\"wb\")\npickle.dump(active_train_err_list, pickle_out)\npickle_out.close()\n\n\npickle_out = open(\"/home/mw/work/test1_largest_margin_minimum_testing_accuracy.pickle\",\"wb\")\npickle.dump(active_test_err_list, pickle_out)\npickle_out.close()\n\n# pickle_out = open(\"test1_random_testing_accuracy.pickle\",\"wb\")\n# pickle.dump(new_rand_test_err_list, pickle_out)\n# pickle_out.close()","execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i2lc7P-Xdj0-","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"source":"# Random Model"},{"cell_type":"code","metadata":{"id":"JSREmlNhdj0_","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"source":"# random_model2 = Net3()","execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"BZllMw3Ldj0_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1d74e985-5eb3-4b05-c396-6eeed64437c5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"source":"rand_loss_list = []\nrand_acc_list = []\nrand_train_err_list = []\nrand_test_err_list = []\n\nfor trial in range(1):\n    random_model2 = Net3()\n    free_params = sum(p.numel() for p in random_model2.parameters() if p.requires_grad)\n\n    trainstep = 125\n    # Loss and optimizer\n\n    criterion = nn.NLLLoss() #You can modify the loss function\n    optimizer = optim.SGD(random_model2.parameters(), lr=0.005, momentum=0.9, weight_decay=8e-4) #You can change the optimizer\n\n    # Train the model\n\n    total_step = len(trainloader)\n    print(total_step)\n    trial_rand_loss_list = []\n    trial_rand_acc_list = []\n\n    trial_rand_train_err_list = []\n    trial_rand_test_err_list = []\n\n    num_epochs = 450\n\n    for epoch in range(num_epochs):\n        print(\"epoch: \", epoch)\n        total = 0\n        correct = 0\n        randomly_selected = np.random.choice(range(len(list(trainloader))), size=20)\n        for i, (images, labels) in enumerate(trainloader):\n            if i not in randomly_selected:\n                continue\n\n    #         images = images.cuda(async=True)\n    #         labels = labels.cuda(async=True)\n\n            images = Variable(images)\n            labels = Variable(labels)\n            # Run the forward pass\n            # print(\"running forward pass....\")\n            outputs = random_model2(images)\n\n            loss = criterion(outputs, labels)\n            rand_loss_list.append(loss.item())\n\n            # Backprop \n            # print(\"backpropagating......\")\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n             # Track the accuracy\n            total = labels.size(0) + total\n            _, predicted = torch.max(outputs.data, 1)\n            correct = (predicted == labels).sum().item() + correct\n            rand_acc_list.append(correct / total)\n\n\n            if (i + 1) % trainstep == 0:\n                w = torch.nn.utils.parameters_to_vector(random_model2.parameters())\n                print(w)\n                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n                              (correct / total) * 100))\n        trial_rand_train_err_list.append(compute_train_acc(random_model2, trainloader))\n        trial_rand_test_err_list.append(compute_test_acc(random_model2, testloader))\n        if (total == correct):\n            break \n\n    rand_loss_list.append(trial_rand_loss_list)\n    rand_acc_list.append(trial_rand_acc_list)\n    rand_train_err_list.append(trial_rand_train_err_list)\n    rand_test_err_list.append(trial_rand_test_err_list)\n            \nprint('Finished Training') \n\n# rand_loss_list = np.array(rand_loss_list)\n# rand_acc_list = np.array(rand_acc_list)\nrand_train_err_list= np.array(rand_train_err_list)\nrand_test_err_list=np.array(rand_test_err_list)\n\n# rand_loss_list = np.mean(rand_loss_list, axis=0)\n# rand_acc_list = np.mean(rand_acc_list, axis=0)\nrand_train_err_list= np.mean(rand_train_err_list, axis=0)\nrand_test_err_list=np.mean(rand_test_err_list, axis=0)\n\n\n\n","execution_count":18,"outputs":[{"output_type":"stream","text":"75\nepoch:  0\nepoch:  1\nepoch:  2\nepoch:  3\nepoch:  4\nepoch:  5\nepoch:  6\nepoch:  7\nepoch:  8\nepoch:  9\nepoch:  10\nepoch:  11\nepoch:  12\nepoch:  13\nepoch:  14\nepoch:  15\nepoch:  16\nepoch:  17\nepoch:  18\nepoch:  19\nepoch:  20\nepoch:  21\nepoch:  22\nepoch:  23\nepoch:  24\nepoch:  25\nepoch:  26\nepoch:  27\nepoch:  28\nepoch:  29\nepoch:  30\nepoch:  31\nepoch:  32\nepoch:  33\nepoch:  34\nepoch:  35\nepoch:  36\nepoch:  37\nepoch:  38\nepoch:  39\nepoch:  40\nepoch:  41\nepoch:  42\nepoch:  43\nepoch:  44\nepoch:  45\nepoch:  46\nepoch:  47\nepoch:  48\nepoch:  49\nepoch:  50\nepoch:  51\nepoch:  52\nepoch:  53\nepoch:  54\nepoch:  55\nepoch:  56\nepoch:  57\nepoch:  58\nepoch:  59\nepoch:  60\nepoch:  61\nepoch:  62\nepoch:  63\nepoch:  64\nepoch:  65\nepoch:  66\nepoch:  67\nepoch:  68\nepoch:  69\nepoch:  70\nepoch:  71\nepoch:  72\nepoch:  73\nepoch:  74\nepoch:  75\nepoch:  76\nepoch:  77\nepoch:  78\nepoch:  79\nepoch:  80\nepoch:  81\nepoch:  82\nepoch:  83\nepoch:  84\nepoch:  85\nepoch:  86\nepoch:  87\nepoch:  88\nepoch:  89\nepoch:  90\nepoch:  91\nepoch:  92\nepoch:  93\nepoch:  94\nepoch:  95\nepoch:  96\nepoch:  97\nepoch:  98\nepoch:  99\nepoch:  100\nepoch:  101\nepoch:  102\nepoch:  103\nepoch:  104\nepoch:  105\nepoch:  106\nepoch:  107\nepoch:  108\nepoch:  109\nepoch:  110\nepoch:  111\nepoch:  112\nepoch:  113\nepoch:  114\nepoch:  115\nepoch:  116\nepoch:  117\nepoch:  118\nepoch:  119\nepoch:  120\nepoch:  121\nepoch:  122\nepoch:  123\nepoch:  124\nepoch:  125\nepoch:  126\nepoch:  127\nepoch:  128\nepoch:  129\nepoch:  130\nepoch:  131\nepoch:  132\nepoch:  133\nepoch:  134\nepoch:  135\nepoch:  136\nepoch:  137\nepoch:  138\nepoch:  139\nepoch:  140\nepoch:  141\nepoch:  142\nepoch:  143\nepoch:  144\nepoch:  145\nepoch:  146\nepoch:  147\nepoch:  148\nepoch:  149\nepoch:  150\nepoch:  151\nepoch:  152\nepoch:  153\nepoch:  154\nepoch:  155\nepoch:  156\nepoch:  157\nepoch:  158\nepoch:  159\nepoch:  160\nepoch:  161\nepoch:  162\nepoch:  163\nepoch:  164\nepoch:  165\nepoch:  166\nepoch:  167\nepoch:  168\nepoch:  169\nepoch:  170\nepoch:  171\nepoch:  172\nepoch:  173\nepoch:  174\nepoch:  175\nepoch:  176\nepoch:  177\nepoch:  178\nepoch:  179\nepoch:  180\nepoch:  181\nepoch:  182\nepoch:  183\nepoch:  184\nepoch:  185\nepoch:  186\nepoch:  187\nepoch:  188\nepoch:  189\nepoch:  190\nepoch:  191\nepoch:  192\nepoch:  193\nepoch:  194\nepoch:  195\nepoch:  196\nepoch:  197\nepoch:  198\nepoch:  199\nepoch:  200\nepoch:  201\nepoch:  202\nepoch:  203\nepoch:  204\nepoch:  205\nepoch:  206\nepoch:  207\nepoch:  208\nepoch:  209\nepoch:  210\nepoch:  211\nepoch:  212\nepoch:  213\nepoch:  214\nepoch:  215\nepoch:  216\nepoch:  217\nepoch:  218\nepoch:  219\nepoch:  220\nepoch:  221\nepoch:  222\nepoch:  223\nepoch:  224\nepoch:  225\nepoch:  226\nepoch:  227\nepoch:  228\nepoch:  229\nepoch:  230\nepoch:  231\nepoch:  232\nepoch:  233\nepoch:  234\nepoch:  235\nepoch:  236\nepoch:  237\nepoch:  238\nepoch:  239\nepoch:  240\nepoch:  241\nepoch:  242\nepoch:  243\nepoch:  244\nepoch:  245\nepoch:  246\nepoch:  247\nepoch:  248\nepoch:  249\nepoch:  250\nepoch:  251\nepoch:  252\nepoch:  253\nepoch:  254\nepoch:  255\nepoch:  256\nepoch:  257\nepoch:  258\nepoch:  259\nepoch:  260\nepoch:  261\nepoch:  262\nepoch:  263\nepoch:  264\nepoch:  265\nepoch:  266\nepoch:  267\nepoch:  268\nepoch:  269\nepoch:  270\nepoch:  271\nepoch:  272\nepoch:  273\nepoch:  274\nepoch:  275\nepoch:  276\nepoch:  277\nepoch:  278\nepoch:  279\nepoch:  280\nepoch:  281\nepoch:  282\nepoch:  283\nepoch:  284\nepoch:  285\nepoch:  286\nepoch:  287\nepoch:  288\nepoch:  289\nepoch:  290\nepoch:  291\nepoch:  292\nepoch:  293\nepoch:  294\nepoch:  295\nepoch:  296\nepoch:  297\nepoch:  298\nepoch:  299\nepoch:  300\nepoch:  301\nepoch:  302\nepoch:  303\nepoch:  304\nepoch:  305\nepoch:  306\nepoch:  307\nepoch:  308\nepoch:  309\nepoch:  310\nepoch:  311\nepoch:  312\nepoch:  313\nepoch:  314\nepoch:  315\nepoch:  316\nepoch:  317\nepoch:  318\nepoch:  319\nepoch:  320\nepoch:  321\nepoch:  322\nepoch:  323\nepoch:  324\nepoch:  325\nepoch:  326\nepoch:  327\nepoch:  328\nepoch:  329\nepoch:  330\nepoch:  331\nepoch:  332\nepoch:  333\nepoch:  334\nepoch:  335\nepoch:  336\nepoch:  337\nepoch:  338\nepoch:  339\nepoch:  340\nepoch:  341\nepoch:  342\nepoch:  343\nepoch:  344\nepoch:  345\nepoch:  346\nepoch:  347\nepoch:  348\nepoch:  349\nepoch:  350\nepoch:  351\nepoch:  352\nepoch:  353\nepoch:  354\nepoch:  355\nepoch:  356\nepoch:  357\nepoch:  358\nepoch:  359\nepoch:  360\nepoch:  361\nepoch:  362\nepoch:  363\nepoch:  364\nepoch:  365\nepoch:  366\nepoch:  367\nepoch:  368\nepoch:  369\nepoch:  370\nepoch:  371\nepoch:  372\nepoch:  373\nepoch:  374\nepoch:  375\nepoch:  376\nepoch:  377\nepoch:  378\nepoch:  379\nepoch:  380\nepoch:  381\nepoch:  382\nepoch:  383\nepoch:  384\nepoch:  385\nepoch:  386\nepoch:  387\nepoch:  388\nepoch:  389\nepoch:  390\nepoch:  391\nepoch:  392\nepoch:  393\nepoch:  394\nepoch:  395\nepoch:  396\nepoch:  397\nepoch:  398\nepoch:  399\nepoch:  400\nepoch:  401\nepoch:  402\nepoch:  403\nepoch:  404\nepoch:  405\nepoch:  406\nepoch:  407\nepoch:  408\nepoch:  409\nepoch:  410\nepoch:  411\nepoch:  412\nepoch:  413\nepoch:  414\nepoch:  415\nepoch:  416\nepoch:  417\nepoch:  418\nepoch:  419\nepoch:  420\nepoch:  421\nepoch:  422\nepoch:  423\nepoch:  424\nepoch:  425\nepoch:  426\nepoch:  427\nepoch:  428\nepoch:  429\nepoch:  430\nepoch:  431\nepoch:  432\nepoch:  433\nepoch:  434\nepoch:  435\nepoch:  436\nepoch:  437\nepoch:  438\nepoch:  439\nepoch:  440\nepoch:  441\nepoch:  442\nepoch:  443\nepoch:  444\nepoch:  445\nepoch:  446\nepoch:  447\nepoch:  448\nepoch:  449\nFinished Training\n","name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WZfbPsdadj0_","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"source":"## Compare Largest Margin to Random"},{"cell_type":"code","metadata":{"id":"8NmlTnjgdj1A","colab":{"base_uri":"https://localhost:8080/","height":295},"outputId":"6dedae48-4069-439c-a3a4-3c2cb0ff4773","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"source":"new_active_train_err_list = [0]\nnew_active_train_err_list.extend(active_train_err_list)\n# plt.plot(new_active_train_err_list)\n# plt.plot(active_test_err_list)\nnew_rand_train_err_list = [0]\nnew_rand_train_err_list.extend(rand_train_err_list)\n# plt.plot(new_rand_train_err_list)\n# # plt.plot(active_test_err_list)\n# plt.legend(['active', 'random'])\n# plt.title(\"Active Training Set Accuracy\")\n# plt.xlabel(\"Epoch\")\n# plt.ylabel(\"Training Set Accuracy\")\n# plt.savefig(\"/home/mw/work/test3_4_acc1.png\")\n# plt.show()\n\npickle_out = open(\"/home/mw/work/test1_largest_margin_minimum2_training_accuracy.pickle\",\"wb\")\npickle.dump(new_active_train_err_list, pickle_out)\npickle_out.close()\n\npickle_out = open(\"/home/mw/work/test1_random2_training_accuracy.pickle\",\"wb\")\npickle.dump(new_rand_train_err_list, pickle_out)\npickle_out.close()","execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"n9Tpwxn5dj1A","colab":{"base_uri":"https://localhost:8080/","height":295},"outputId":"a7887c8e-8894-425a-efda-1ceb008eb255","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"source":"new_active_test_err_list = [0]\nnew_active_test_err_list.extend(active_test_err_list)\n\n# plt.plot(new_second_train_err_list)\n# plt.plot(active_test_err_list)\nnew_rand_test_err_list = [0]\nnew_rand_test_err_list.extend(rand_test_err_list)\n\n# plt.plot(new_active_test_err_list)\n# # plt.plot(active_test_err_list)\n# plt.plot(new_rand_test_err_list)\n# # plt.plot(active_test_err_list)\n# plt.legend(['active', 'random'])\n# plt.title(\"Active Test Set Accuracy\")\n# plt.xlabel(\"Epoch\")\n# plt.ylabel(\"Test Set Accuracy\")\n# plt.savefig(\"/home/mw/work/test3_4_acc2.png\")\n# plt.show()\n\npickle_out = open(\"/home/mw/work/test2_largest_margin_minimum2_testing_accuracy.pickle\",\"wb\")\npickle.dump(new_active_test_err_list, pickle_out)\npickle_out.close()\n\npickle_out = open(\"/home/mw/work/test2_random2_testing_accuracy.pickle\",\"wb\")\npickle.dump(new_rand_test_err_list, pickle_out)\npickle_out.close()","execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"C_AW9RKUdj1A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"source":"# ... after training, save your model \nmodel_filename = '/home/mw/work/random_model2.pt'\ntorch.save(random_model2.state_dict(), model_filename)\n\n# .. to load your previously training model:\n# model2 = Net3()\n# model2.load_state_dict(torch.load(model_filename))\n# model2.eval()","execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"EV3yKX6Ndj1A","colab":{"base_uri":"https://localhost:8080/","height":295},"outputId":"b989a58f-07c2-44f9-ff0a-053b7351fab7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"source":"plt.plot(active_train_err_list)\n# plt.plot(active_test_err_list)\nplt.plot(rand_train_err_list)\n# plt.plot(active_test_err_list)\nplt.legend(['active', 'random'])\nplt.title(\"Active Training Set Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Training Set Accuracy\")\nplt.savefig(\"/home/mw/work/test2_acc1_min.png\")\nplt.show()","execution_count":38,"outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/upload/rt/EV3yKX6Ndj1A/qwspwqqtwq.png\">"}}]}]}